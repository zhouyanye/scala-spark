flatMap()方法转换数据 对集合中的每个元素进行map操作再扁平化
sortBy(f:(T)=>K,ascending,numPartitions)方法进行排序 f:(T)=>K 左边是输入的RDD(是要被排序对象中的每一个元素)，右边是输出的RDD(元素中要进行排序的值) ascending：指定排序顺序，true表示升序(默认)，false表示降序。numPartitions：决定排序后的RDD分区个数,默认排序后的分区个数和排序之前的个数相同
take(N)方法取前N个元素,返回一个数组， take与collect方法不同的是，take方法返回的是一个数组(指定取多少个元素)，collect方法是将整个RDD的所有元素收集到驱动程序程序中，返回的是一个数组(获取全部数据)
mapPartitions(f:(Int)=>Iterator[T])方法将RDD中的每个分区转换为一个迭代器，然后将迭代器中的元素收集到一个数组中，返回一个数组
filter(f:(T)=>Boolean)方法过滤RDD中的元素，返回一个RDD
distinct()方法去重，返回一个RDD
union(other:RDD[T])方法将两个RDD中的元素合并成一个RDD，返回一个RDD(合集) 元素类型要一致
intersect(other:RDD[T])方法求两个RDD的交集，返回一个RDD
subtract(other:RDD[T])方法求两个RDD的补集，返回一个RDD(去除相同元素) 两个RDD顺序会影响结果
cartesian(other:RDD[T])方法将两个RDD中的元素进行笛卡尔积，返回一个RDD
reduce(f:(T,T)=>T)方法对RDD中的元素进行累加，返回一个T


makeRDD:

makeRDD是SparkContext类中的一个方法，用于创建一个普通的RDD，其中每个元素都是一个分区中的数据。这个方法不关心元素之间的关系，只是简单地将数据分布到各个分区中。以下是一个简单的示例：
import org.apache.spark.{SparkConf, SparkContext}

val conf = new SparkConf().setAppName("Example").setMaster("local")
val sc = new SparkContext(conf)

// 创建一个包含1到10的整数的RDD，分成两个分区
val rdd = sc.makeRDD(1 to 10, 2)

// 打印RDD的内容
rdd.foreach(println)

sc.stop()
pairRDD是指包含键值对（key-value pairs）的RDD，其中每个元素都是一个键值对。这种类型的RDD常用于进行分布式的键值对操作，例如groupByKey，reduceByKey等。以下是一个简单的示例：
import org.apache.spark.{SparkConf, SparkContext}

val conf = new SparkConf().setAppName("Example").setMaster("local")
val sc = new SparkContext(conf)

// 创建一个包含键值对的RDD
val pairRDD = sc.parallelize(Seq(("a", 1), ("b", 2), ("a", 3), ("b", 4)))

// 对具有相同键的值进行求和
val sumByKey = pairRDD.reduceByKey(_ + _)

// 打印结果
sumByKey.foreach(println)

sc.stop()


